---
layout: page
title: Distillation for High-Quality Knowledge Extraction via Explainable Oracle Approach
---

## Knowledge Distillation이란

Knowledge distillation의 가장 대표적인 목적은 모델 경량화입니다. 이를 위해, 크고 복잡한 모델(일명 *teacher model*)의 지식을 보다 작고 효율적인 모델(일명 *student model*)로 전달하는 과정을 거치게 됩니다. 이 때 *student model*은 *teacher model*과 동일한 output을 내도록 학습되며 이 과정을 통해 우리는 규모가 큰 *teacher model*을 규모가 작은 *student model*로 압축하여 경량화 효과를 얻을 수 있습니다. 
